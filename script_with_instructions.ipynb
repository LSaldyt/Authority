{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. JSTOR data to be parsed to be placed under data folder.\n",
    "2. The database jstor-authority.db to be placed under database folder\n",
    "\n",
    "These are the steps to perform authority on the JSTOR dataset.(Note: Commands to run each step are written seperately after this cell)\n",
    "\n",
    "A. Parsing JSTOR data.\n",
    "1. Parse all the JSTOR xml files to the database. \n",
    "2. Mesh content is placed in Authority/r_table/parser/results/mesh in <journal-name>.txt format.\n",
    "3. Get mesh terms for the above content by running Authority/r_table/parser/fetch_mesh_terms.bat. ( Set input and output paths in the bat file. currently only batch script is available)\n",
    "4. Store mesh terms in the database by executing Authority/r_table/parser/mesh_id_parser/mesh_output_parser.py\n",
    "\n",
    "B. Creating r_table.\n",
    "1. create reference sets - attribute match, non match and name set mixed.\n",
    "2. compute similarity profiles and compute r\n",
    "3. Run smoothening in matlab\n",
    "4. Post processing r-values(interpolation and extrapolation)\n",
    "\n",
    "C. Clustering\n",
    "1. perform clustering.\n",
    "\n",
    "D. Gold standard databases:\n",
    "1. Create google_scholar, bio diversity heritage and jstor self citation db.\n",
    "\n",
    "E. Evaluation:\n",
    "1. evaluate using the above gold standard data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r r_table/requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "________________________________________________________________________________________________\n",
    "A. Parsing JSTOR data\n",
    "1. Parse all the JSTOR xml files to the database.s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd r_table/parser\n",
    "!python main.py  --zip_file \"../../data/receipt-id-561931-jcodes-ab-part-001.zip\"\n",
    "!python main.py  --zip_file \"../../data/receipt-id-561931-jcodes-ab-part-002.zip\"\n",
    "!python main.py  --zip_file \"../../data/receipt-id-561931-jcodes-cdef-part-001.zip\"\n",
    "!python main.py  --zip_file \"../../data/receipt-id-561931-jcodes-cdef-part-002.zip\"\n",
    "!python main.py  --zip_file \"../../data/receipt-id-561931-jcodes-ghij-part-001.zip\"\n",
    "!python main.py  --zip_file \"../../data/receipt-id-561931-jcodes-ghij-part-002.zip\"\n",
    "!python main.py  --zip_file \"../../data/receipt-id-561931-jcodes-klmnop-part-001.zip\"\n",
    "!python main.py  --zip_file \"../../data/receipt-id-561931-jcodes-klmnop-part-002.zip\"\n",
    "!python main.py  --zip_file \"../../data/qrstuvwyz.zip\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Mesh content is placed in Authority/r_table/parser/results/mesh in <journal-name>.txt format. \n",
    "3. Download the jar file from https://ii.nlm.nih.gov/Web_API/ and place it under mesh_id_parser folder. Follow the instructions from https://ii.nlm.nih.gov/Web_API/ and register your email. Make necessary changes like providing email and password in the GenericBatch.java.\n",
    "4. Get mesh terms for the above content by running Authority/r_table/parser/fetch_mesh_terms.bat or .sh file ( Set input and output paths in the bat/sh file and set web_api_examples_path to the \"SKR_Web_API_V2_3\\SKR_Web_API_V2_3\\examples\" folder )\n",
    "5. Store mesh terms using below command.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd mesh_id_parser/\n",
    "!python mesh_output_parser.py --file ../results/mesh/output/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____________________________________________________________________________________________\n",
    "B. Creating r_table.\n",
    "1. create reference sets - attribute match, non match and name set mixed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ../reference_sets/\n",
    "!python delete_sets.py\n",
    "!python create_sets.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. compute similarity profiles and compute r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ../compute_r/\n",
    "!python compute_similarity.py\n",
    "!python compute_r.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Results of above commands are placed under computer_r/results folder. Similarity profiles are stored in x<score_attribute>\\_m.json and x<score_attribute>\\_nm.json files. R-table value are stored in r_x<score_attribute>.json.\n",
    "2. Smoothen r values by running matlab script. smoothing_quadratic.m\n",
    "3. final results will be in results/r_smoothen.txt\n",
    "4. Note: This is done on a different node.\n",
    " ____________________________________________________________________\n",
    "5. Interpolate r values by running post processing script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!python post_processing_r.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "copy resultant r_x1.json, r_x2.json, r_final.json and upper_profiles.txt from results folder to clustering folder for clustering.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd results\n",
    "%cp r_x1.json r_x2.json r_final.json upper_profiles.txt r_x10.json nicknames.json ../../../clustering/r_table/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C. Clustering\n",
    "1. perform clustering\n",
    "\n",
    "2. The script script.sh performs clustering on the blocks paralelly to speed up the process. Command to use python main_firstinitial.py <starting_block_number> <ending_block number>. By running the command, you perform clustering on blocks starting from <starting_block_number> to <ending_block>. Adjust the parallelization according to your compute speeds.\n",
    "\n",
    "3. The script store.sh combines the results obtained from above parallel computing and stores them in the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ../../../clustering/\n",
    "!./script.sh\n",
    "!./store.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "D. Evaluation. \n",
    "1. Google scholar. Results will be placed in evaluation/google_scholar/evaluation_results_gs.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ../evaluation/google scholar/\n",
    "!python evaluate_gs.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Self citations. Results will be placed in evaluation/self-citations/final_eval_results_self.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ../self-citations/\n",
    "!./script.sh\n",
    "!python combine_results.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. BHL. Results will be placed in evaluation/bhl/evaluation_results_bhl.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ../bhl/\n",
    "!python evaluate_bhl.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code to compute metrics.\n",
    "tp = 6995\n",
    "fp = 477\n",
    "fn = 6\n",
    "tn = 49\n",
    "s= tp+fp+tn+fn\n",
    "accuracy = (tp+tn)/s\n",
    "print(\"bhl results:\")\n",
    "print(\"accuracy : \",accuracy)\n",
    "pairwise_precision = tp/(tp+fp)\n",
    "print(\"pairwise_precision : \",pairwise_precision)\n",
    "pairwise_recall = tp/(tp+fn)\n",
    "print(\"pairwise_recall : \",pairwise_recall)\n",
    "pairwise_f1 = (2*pairwise_precision*pairwise_recall)/(pairwise_recall+pairwise_precision)\n",
    "print(\"pairwise_f1 : \",pairwise_f1)\n",
    "ler = fp/(tp+fp)\n",
    "print(\"pairwise lumping error rate : \",ler)\n",
    "ser = fn/(tn+fn)\n",
    "print(\"pairwise splitting error rate : \",ser)\n",
    "er = (fp+fn)/s\n",
    "print(\"pairwise error rate : \",er)\n",
    "\n",
    "#google scholar\n",
    "tp = 291938\n",
    "fp = 18028\n",
    "fn = 238\n",
    "tn = 5198\n",
    "s= tp+fp+tn+fn\n",
    "\n",
    "accuracy = (tp+tn)/s\n",
    "print(\"gs results:\")\n",
    "print(\"accuracy : \",accuracy)\n",
    "pairwise_precision = tp/(tp+fp)\n",
    "print(\"pairwise_precision : \",pairwise_precision)\n",
    "pairwise_recall = tp/(tp+fn)\n",
    "print(\"pairwise_recall : \",pairwise_recall)\n",
    "pairwise_f1 = (2*pairwise_precision*pairwise_recall)/(pairwise_recall+pairwise_precision)\n",
    "print(\"pairwise_f1 : \",pairwise_f1)\n",
    "ler = fp/(tp+fp)\n",
    "print(\"pairwise lumping error rate : \",ler)\n",
    "ser = fn/(tn+fn)\n",
    "print(\"pairwise splitting error rate : \",ser)\n",
    "er = (fp+fn)/s\n",
    "print(\"pairwise error rate : \",er)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Py3-basic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
